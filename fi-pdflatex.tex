%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% I, the copyright holder of this work, release this work into the
%% public domain. This applies worldwide. In some countries this may
%% not be legally possible; if so: I grant anyone the right to use
%% this work for any purpose, without any conditions, unless such
%% conditions are required by law.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[
  digital,     %% The `digital` option enables the default options for the
               %% digital version of a document. Replace with `printed`
               %% to enable the default options for the printed version
               %% of your thesis.
%%  color,       %% Uncomment these lines (by removing the %% at the
%%               %% beginning) to use color in the printed version of your
%%               %% document
  oneside,     %% The `oneside` option enables one-sided typesetting,
               %% which is preferred if you are only going to submit a
               %% digital version of your thesis. Replace with `twoside`
               %% for double-sided typesetting if you are planning to
               %% also print your thesis. For double-sided typesetting,
               %% use at least 120 g/m² paper to prevent show-through.
  nosansbold,  %% The `nosansbold` option prevents the use of the
               %% sans-serif type face for bold text. Replace with `sansbold` to use sans-serif type face for bold text.
  nocolorbold, %% The `nocolorbold` option disables the usage of the
               %% blue color for bold text, instead using black. Replace with `colorbold` to use blue for bold text.
  lof,         %% The `lof` option prints the List of Figures. Replace
               %% with `nolof` to hide the List of Figures.
  lot,         %% The `lot` option prints the List of Tables. Replace
               %% with `nolot` to hide the List of Tables.
]{fithesis4}
%% The following section sets up the locales used in the thesis.
\usepackage[resetfonts]{cmap} %% We need to load the T2A font encoding
\usepackage[T1,T2A]{fontenc}  %% to use the Cyrillic fonts with Russian texts.

\usepackage{hyperref}
\hypersetup{
    colorlinks,
    linkcolor={black!50!black},
    citecolor={black!50!black},
    urlcolor={black!80!black}
}

\usepackage[
  main=english, %% By using `czech` or `slovak` as the main locale
                %% instead of `english`, you can typeset the thesis
                %% in either Czech or Slovak, respectively.
  english, german, czech, slovak %% The additional keys allow
]{babel}        %% foreign texts to be typeset as follows:
%%
%%   \begin{otherlanguage}{german}  ... \end{otherlanguage}
%%   \begin{otherlanguage}{czech}   ... \end{otherlanguage}
%%   \begin{otherlanguage}{slovak}  ... \end{otherlanguage}
%%
%%
%% The following section sets up the metadata of the thesis.
\thesissetup{
    date        = \the\year/\the\month/\the\day,
    university  = mu,
    faculty     = fi,
    type        = mgr,
    department  = ,
    author      = {Bc. Maxmilián Šeffer},
    gender      = m,
    advisor     = {doc. Ing. Václav Oujezský, Ph.D.},
    title       = {Voice Work Record Application},
    TeXtitle    = {Voice Work Record Application},
    keywords    = {audio transcription, time tracking, mobile application, cloud computing, voice recognition, natural language processing, generative AI, user experience, offline functionality, multilingual support},
    TeXkeywords = {audio transcription, time tracking, mobile application, cloud computing, voice recognition, natural language processing, generative AI, multilingual support, \ldots},
    abstract    = {%
        This thesis explores the potential of audio transcription technology to simplify time tracking for employees working in the field. The goal is to develop a cloud-based application that allows employees to log their working hours by recording audio descriptions of their tasks, specifying the client and project. This recorded speech is then transcribed into text and converted into structured database entries. The research evaluates the reliability and practicality of this approach in real-world conditions.
        
        In addition to automated transcription, the application provides features for replaying audio recordings, manually editing and managing entries, and exporting data for managerial review. By simplifying the logging process, the solution aims to minimize forgetfulness of employees and improve accuracy of work record while offering employees an efficient way to track their work.
    },
    thanks      = {%
      I would like to thank doc. Ing. Václav Oujezský, Ph.D., for his guidance and leadership throughout this thesis. My thanks also go to Bc. Ondřej Zelinka, with whom I collaborated as he was working simultaneously on a web version of the front-end. I am also grateful to Ing. Roman Kalous, Ph.D. for his support and cooperation on behalf of the DevBalance s.r.o. company.
    },
    bib         = example.bib,
    %% Remove the following line to use the JVS 2018 faculty logo.
    facultyLogo = fithesis-fi,
}
\usepackage{makeidx}      %% The `makeidx` package contains
\makeindex                %% helper commands for index typesetting.

%% These additional packages are used within the document:
\usepackage[acronym]{glossaries}          %% The `glossaries` package
\renewcommand*\glspostdescription{\hfill} %% contains helper commands
\loadglsentries{example-terms-abbrs.tex}  %% for typesetting glossaries
\makenoidxglossaries                      %% and lists of abbreviations.

\usepackage{paralist} %% Compact list environments
\usepackage{amsmath}  %% Mathematics
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{url}      %% Hyperlinks
\usepackage{markdown} %% Lightweight markup
\usepackage{listings} %% Source code highlighting
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows}
\tikzstyle{arrow} = [thick,->,>=stealth]
\usepackage{xcolor}


%% Thesis specific language/framework code highlighting
\lstdefinelanguage{Firestore}{
  morekeywords={
    service, match, allow, read, write, if, null, rules_version
  },
  sensitive=true,
  morecomment=[l]{//},
  morestring=[b]',
  morestring=[b]",
}
\lstdefinelanguage{Dart}{
  keywords={
    abstract, as, assert, async, await, break, case, catch, class, const, continue,
    covariant, default, deferred, do, dynamic, else, enum, export, extends, extension,
    external, factory, false, final, finally, for, Function, get, hide, if, implements,
    import, in, interface, is, late, library, mixin, new, null, on, operator, part,
    required, rethrow, return, set, show, static, super, switch, sync, this, throw,
    true, try, typedef, var, void, while, with, yield
  },
  keywords=[2]{@override, @JsonSerializable, @deprecated, @protected, @required, @visibleForTesting}, % Group 2: annotations
  sensitive=true,
  comment=[l]{//},
  morecomment=[s]{/*}{*/},
  morestring=[b]",
  morestring=[b]',
}
\lstset{
  basicstyle      = \ttfamily,
  identifierstyle = \color{black},
  keywordstyle    = \color{blue},
  keywordstyle    = {[2]\color{cyan}},
  keywordstyle    = {[3]\color{olive}},
  stringstyle     = \color{teal},
  commentstyle    = \itshape\color{magenta},
  breaklines      = true,
  showstringspaces=false,
  frame=lrtb,
}

\usepackage{floatrow} %% Putting captions above tables
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\floatsetup[table]{capposition=top}
\usepackage[babel]{csquotes} %% Context-sensitive quotation marks

%% Used for defining use cases
\usepackage{algorithm}
\usepackage{algpseudocode}

\usepackage{placeins}

\begin{document}
\printnoidxglossary[type=\acronymtype, title=List of Abbreviations]
%% The \chapter* command can be used to produce unnumbered chapters:
\chapter*{Introduction}
%% Unlike \chapter, \chapter* does not update the headings and does not
%% enter the chapter to the table of contents. If we want correct
%% headings and a table of contents entry, we must add them manually:
\markright{\textsc{Introduction}}
\addcontentsline{toc}{chapter}{Introduction}
All workplaces require some form of maintaining records of work performed. For smaller companies a simple solution is often sufficient. This could range from a paper notebook for employees to write in, to a simple web application or an Excel table. However, many companies have a large number of clients, different projects and employees. There is a wide variety of work logging solutions currently available –- ranging from personal time trackers to enterprise-grade software. For generic use cases, work logging is a solved problem. Still, many of these tools fail to address the needs of companies with a large number of clients and field employees. In such cases, workers can handle multiple clients and tasks per day and only log their hours at the end of their shifts. This practice can lead to incomplete or inaccurate records, as employees may forget details or fail to log time accurately, which may later impact client invoicing or internal reporting.

In the past few years, there has been a significant rise of generative \gls{AI} text models and transcription services. \gls{AI} text \acrshort{API}s have become better quality, as well as  much faster and affordable. The thesis will attempt to integrate this technology into a work logging solution to simplify it for employees and concurrently provide more accurate data for employers.

One already used and widely adopted approach is to allow users to record audio notes describing their work, which they can review later. The current state of such solutions often still requires manual intervention to input the data (such as time and work description) into structured formats.

This thesis proposes an enhancement to conventional audio-logging systems by integrating automatic speech transcription and converting the transcribed content directly into structured database entries. The objective is to minimize user effort and maximize the accuracy and immediacy of work logging.

The goal of this thesis is two-folds:
\begin{itemize}
    \item To design and implement a standalone mobile application that supports standard work and time logging functionalities, along with the capability to record audio descriptions of work and automatically convert them into database entries. The application will be developed in collaboration with the company DevBalance s.r.o. (further only referred to as DevBalance), and will be tailored to suit company-wide deployment.
    \item To evaluate whether using transcription services and chat-based \gls{AI} models is a viable approach for this use case. This includes evaluating their performance in terms of transcription speed, precision of data extraction, and the overall cost implications of using such technologies.
\end{itemize}

\chapter{System Requirements}

Before building a useful audio transcription application for time tracking, it’s important to start with a solid requirements analysis. This step helps everyone involved get on the same page about what the system should do, how well it should do it, and any limits it needs to work within. As Wiegers and Beatty explain, defining both functional and non-functional requirements early on helps make sure the software meets user needs and supports business goals \cite{wiegers13}. It also makes it easier to set priorities, stay on track with the project scope, and handle any changes that come up. Some of the key things to focus on during this stage include talking to stakeholders, outlining user scenarios, setting quality expectations, and understanding where and how the system will be used.

The goal of this analysis is to create some sort of structure that guides the development of the Voice Work Record Application – further referred to simply as the \textit{the app}. The system requirements were set through discussions with DevBalance consultant and this thesis's advisor. The analysis highlights both explicit and implicit needs, as well as potential challenges that may arise during implementation.

\section{Users}

The primary users of the Voice Work Record Application are field employees. These users frequently switch between tasks and clients throughout the day. Their primary need is a quick, reliable, and intuitive method to log their working hours and task details in real-time. They benefit from hands-free audio logging that ensures accuracy and minimizes forgotten details. Of course, the application should not prevent creating and adjusting work records manually, it is simply perceived that employees working in an office for a single client will not benefit from the intended voice-recording approach.

\section{Functional Requirements}

The functional requirements define the core capabilities of the Voice Work Record Application, ensuring that it meets user expectations and performs its intended tasks efficiently. These requirements specify the system's behavior in terms of tasks that users can perform.

This section also integrates the functional requirements as outlined by DevBalance. These include support for multiple languages, voice-to-text processing, and generative \gls{AI} for creating short descriptions.

To clearly outline the expected functionality, the requirements are presented in the form of use cases. Each use case describes a specific interaction between the user and the system, defines the prerequisites, the sequence of steps, and the expected outcome. Some use cases could be decomposed into more detailed sub-cases; the following overview keeps them at a high level to maintain readability.

The core functional requirements of the Voice Work Record Application revolve around efficient, voice-enabled time tracking, cloud-based data storage, secure authentication, and multi-language support. The application must accommodate hands-free logging while ensuring that users can review, edit, and manage their work records with minimal friction. Additionally, data security mechanisms must be in place to guarantee data integrity and user privacy. The system is not meant to be used purely in an offline environment, but then again it should not be synchronously dependent on an internet connection – some degree of offline use should be allowed.

\subsection{Voice-to-Text Recording and Storage}

\noindent \textbf{Prerequisites:} User is authenticated, stable internet connection

\noindent \textbf{Description:}  
Employees can record audio descriptions of their work in either Czech or English. The system transcribes the audio into text, uses a trained \gls{AI} model to extract key information, and stores the recording in a cloud-based environment. The extracted data is parsed into a structured database entry, first saved locally and then persistently in the cloud.

\noindent \textbf{Steps:}
\begin{enumerate}
    \item User navigates to the new recording screen.
    \item User initiates an audio recording in the application.
    \item User verbally describes the work performed, using either Czech or English.
    \item System uploads the audio recording to the cloud.
    \item Transcription service processes the audio and converts it into text.
    \item \gls{AI} model analyzes the transcribed text, extracting key information such as client, activity, project and work duration.
    \item System structures the extracted data into a work record.
    \item Work record is temporarily saved locally on the device.
    \item Work record is uploaded to a cloud-based environment.
    \item The employee receives a notification upon completion of the transcription and data entry.
\end{enumerate}

\subsection{Adding Work Records Manually}

\noindent \textbf{Prerequisites:} User is authenticated, stable internet connection

\noindent \textbf{Description:}  
Employees can manually enter work records if they prefer not to describe the work verbally.

\noindent \textbf{Steps:}
\begin{enumerate}
    \item User navigates to the new recording screen.
    \item User initiates manual work record entry.
    \item User inputs details such as client, activity, project, task description and duration of work.
    \item Work record is uploaded to a cloud-based environment.
\end{enumerate}

\subsection{Reviewing and Editing Work Entries}

\noindent \textbf{Prerequisites:} User is authenticated, work entries exist, stable internet connection (unless previously viewed)

\noindent \textbf{Description:}  
Employees can review and edit their past work records, including both manually entered and transcribed entries.

\noindent \textbf{Steps:}
\begin{enumerate}
    \item User navigates to the screen with past work records.
    \item User selects a work entry to review.
    \item User can replay the original audio recording (if available).
    \item User can see the full transcription of the audio recording (if available).
    \item User can edit the work record. Specifically, the client, activity, project, task description, duration of work. User is not allowed to adjust neither the audio recording and the literal transcription.
    \item Work record is uploaded to a cloud-based environment.
\end{enumerate}

\subsection{Searching, Filtering, and Sorting Work Entries}

\noindent \textbf{Prerequisites:} User is authenticated, work entries exist, stable internet connection

\noindent \textbf{Description:}  
Employees can search for and filter work logs based on the following criteria:
\begin{itemize}
    \item Date
    \item Project
    \item Client
    \item Activity
\end{itemize}

\noindent \textbf{Steps:}
\begin{enumerate}
    \item User navigates to the screen with past work records.
    \item User pushes the search and filter buttons.
    \item User selects criteria for filtering
    \item User selects criteria for sorting
    \item Matching work entries are displayed, together with sums of worked hours according to selected group by settings. Work entries in different groups are also visually separated. 
\end{enumerate}

\subsection{User Authentication and Access Control}

\noindent \textbf{Prerequisites:} None

\noindent \textbf{Description:}  
Authentication and access control ensure that only authorized users can log in and access their respective data.

\noindent \textbf{Steps:}
\begin{enumerate}
    \item User attempts to log in via Microsoft authentication.
    \item User's credentials are verified through Firebase Authentication.
    \item Employee’s email domain is verified.
    \item If the domain is authorized, the employee is granted access to his/her past work records only – not to work records of any other employee.
    \item System ensures that employees from different domains cannot view or modify each other's records.
\end{enumerate}

\subsection{Offline Audio Logging and Synchronization}

\noindent \textbf{Actors:} Employee

\noindent \textbf{Prerequisites:} User is authenticated, no internet connection available

\noindent \textbf{Description:}  
Employees can record work descriptions offline. Once an internet connection is restored, the system automatically uploads and processes the recordings, ensuring that data is synchronized across devices.

From the user perspective, the steps are identical to when internet connection is available, hence the simplified steps.

\noindent \textbf{Steps:}
\begin{enumerate}
    \item Employee records an audio entry while offline.
    \item System stores the recording locally on the device.
    \item In a reasonable time frame, once internet connection is restored, the system automatically uploads and processes the recording.
    \item Work record is uploaded to a cloud-based environment.
\end{enumerate}

\subsection{Receiving Notifications}

\noindent \textbf{Prerequisites:} User is authenticated, relevant event occurs

\noindent \textbf{Description:}  
Employees receive notifications when important events occur, such as transcription completion or record updates. Notification allows the user to directly open the work record, inspect it, and confirm its correctness (or immediately fix incorrect attributes).

\noindent \textbf{Steps:}
\begin{enumerate}
    \item System detects an event requiring a notification (e.g., transcription complete).
    \item System generates a notification message.
    \item User receives an in-app notification.
    \item User can directly open, inspect and edit the work record.
\end{enumerate}

\subsection{Setting User Interface Language}

\noindent \textbf{Prerequisites:} User is authenticated

\noindent \textbf{Description:}  
Users can switch between Czech and English for the user interface, allowing the application to support a multilingual user base.

\noindent \textbf{Steps:}
\begin{enumerate}
    \item User navigates to the settings menu.
    \item User selects the preferred language (Czech or English).
    \item The user interface updates to the selected language.
\end{enumerate}

\subsection{Setting User Interface Theme}

\noindent \textbf{Prerequisites:} User is authenticated

\noindent \textbf{Description:}  
Users can adjust common quality of life preferences, such as theme of the application.

\noindent \textbf{Steps:}
\begin{enumerate}
    \item User navigates to the settings menu.
    \item User selects the preferred theme (light or dark).
    \item The user interface updates to reflect the selected theme.
\end{enumerate}

\section{Non-Functional Requirements}

While functional requirements define what the system must do, non-functional requirements ensure that the system operates effectively under real-world conditions. These requirements specify qualities such as performance, security, usability, and scalability, ensuring that the application remains reliable, efficient, and user-friendly over time.  

It has been decided fairly early on that the Voice Work Record Application will be at its core a cloud-based solution. One of the requirements being the need for seamless data synchronization and high availability, while ensuring that sensitive work records remain secure and private. With a desktop-optimized web interface being developed concurrently with this mobile interface, it is essential that the user has the option to switch between the interfaces seamlessly should the need arise.

The following sections outline the essential characteristics that the system must meet to provide a smooth, secure, and scalable experience.

\subsection{Performance and Reliability}  
The transcription process does not need to be instant, but users should be notified upon completion so they can review and verify the results. The system must handle multiple concurrent users and organizations without performance degradation.

While a constant internet connection is not required, users must be able to record audio offline. The recorded audio should be queued and automatically processed when the device reconnects to the internet. Additionally, the system should ensure that no data is lost in case of connection interruptions.

\subsection{Usability and Accessibility}  
The user interface should be designed with simplicity and ease of use in mind, requiring minimal training. Hands-free operation should be a priority, with a very small number of interactions needed to initialize audio recording and create new work records. Simple touch interactions should allow users to log work efficiently without needing extensive manual input.

The system should also provide clear visual feedback to confirm actions. Text should be minimized for core functionality but available for further information on how to use the system - should the user request it.

\subsection{Security and Compliance}  
All user data must be securely stored and transmitted using encryption. The application must ensure to a reasonable degree that users’ personal and work-related information is not exposed or misused. A reasonable degree is mentioned due to existing plans to use third-party authentication that might not be \gls{GDPR} compliant (Firebase Authentication, for example, has US only servers).

Additionally, access control mechanisms must prevent unauthorized users from accessing or modifying work records.

\subsection{Scalability}  
The system must be designed to support a growing number of users and organizations. As adoption increases, performance should remain stable, and the backend infrastructure must be capable of handling increased storage and processing demands without requiring significant downtime or maintenance.

\subsection{Cross-Platform Support}  
The application must be available on both Android and \gls{iOS}\footnote{DevBalance does not plan to provide an Apple Developer Subscription during the development and testing phases. Therefore, it is considered sufficient for the final version to be tested internally by the developer, with no need to distribute it to testers for the time being.} platforms, ensuring that employees can access and use the system regardless of their device. The mobile application should maintain consistent functionality and usability across different devices and operating system versions.

\subsection{Logging}  
The system must maintain detailed logs for key operations, including the creation and modification of work records. Specifically, changes to work records and calling external services such as authentication events must be logged.

\subsection{Failure Handling and Reporting}  
The application must include mechanisms for detecting and reporting failures. If fatal issue occurs, and the failure must be logged for further debugging.  

A proper feedback loop should be established to automatically report to the development team, enabling proactive maintenance and issue resolution. The logs mentioned above should provide developers with enough insights into potential issues, user errors, or security concerns.

\subsection{Database Support}  
The system should efficiently handle up to a million records and a million audio files without significant performance degradation.  

\subsection{Performance Optimization}  
The system must be optimized for fast data loading and processing, even with large volumes of records. Voice recordings should be quickly accessible for playback, ensuring a responsive user experience.

\subsection{Storage Capacity}  
The system must efficiently manage storage for millions of short voice files of sufficient quality. It should automatically optimize storage usage while ensuring quick access to the files when needed.

\subsection{Voice Recordings Security}  
Voice recordings must be securely encrypted both during transmission and at rest. The system should ensure that sensitive voice data remains protected from unauthorized access or tampering.

\subsection{Multilingual Support}  
The application must support two language variants (Czech and English) for the user interface and for processing text from voice recordings. This includes ensuring that the transcription, \gls{AI} models, and interface components can handle both languages seamlessly.

\subsection{UI Responsiveness}  
The user interface must be responsive and provide a consistent user experience across smartphones and tablets. The application should adapt seamlessly to different screen sizes and resolutions to ensure usability in both small and large devices.

\chapter{Market Analysis}

\section{Introduction}
The need for efficient time-tracking solutions has led to the development of various tools aimed at helping businesses and employees manage work hours effectively. Many existing solutions rely on manual entry. However, there are still gaps in usability, accuracy, and more of a hands-free operation, particularly for field employees who require a more seamless and automated way to log their time. In this chapter we will take a look at mobile applications that come close to our use case. In doing so, strengths and weaknesses of these solutions will be analyzed to either avoid or use some of those patterns in the Voice Work Record Application.

\section{Existing Solutions}

The solutions chosen for analysis are the most prominent mobile applications. Some cater towards business, some more towards individuals. All of the following were used by the developer for a week each to log hours in a day job using Android 15, coincidentally one of the target platforms.

\begin{itemize}
\item \textbf{Clockify}: A widely used time-tracking tool that provides manual entry, timers, and integration with various project management tools. It's powerful in terms of reporting capabilities as well and catered more toward individual use. It features very good user experience and overall fleshed out \gls{UI}.
\item \textbf{Toggl Track}: Offers intuitive time tracking with automation features, but still relies on user interaction to start and stop time logs. It lacks built-in voice transcription for work entry. Some issues with 3rd party authentication were discovered.
\item \textbf{Hubstaff}: Off the bat Hubstaff does not include any 3rd party authentication and only allows Hubstaff specific user accounts. The user experience is not as clean as other solutions, but it's catered more towards businesses, in a similar way as the intended purpose of the Voice Work Record App. Includes \gls{GPS} tracking and partly automated timesheets, but does not focus on voice-based logging. The setup of business owner and team members is well implemented and an established design.
\item \textbf{TSheets by QuickBooks}: Similar to Hubstaff the set of features caters towards businesses. Provides detailed time-tracking features and payroll integration but requires manual input or \gls{GPS} tracking, with no emphasis on voice-based automation.
\end{itemize}

\section{Strengths and weaknesses of existing solutions}

Most existing solutions provide cloud-based tracking, some even integrations with payroll and project management tools. In addition, these applications are often highly optimized and feature rich, ensuring a great user experience. However, many of their most valuable features are restricted behind paywalls, necessitating costly subscriptions. Moreover, businesses frequently lack control over the application's functionality and long-term availability, making them dependent on third-party providers. Organizations require a time-tracking solution that offers reliability and stability without the risk of unexpected feature limitations, forced upgrades, or service discontinuation. 

The majority of existing solutions rely on manual input, timers, or \gls{GPS} tracking, which may be impractical for employees working in dynamic field environments. Although some applications incorporate voice-based logging, these implementations are generally limited to note taking rather than full transcription and structured work entry generation. Consequently, such solutions function more as auxiliary documentation tools rather than complete work-logging systems. According to my personal testing, all of the above work very well for what they are and usually provide a very strong set of features. They simply lack audio transcription. The Voice Work Record Application does not aim to provide the same amount or more features as the above mentioned, it's seen simply as an alternative solution for those whose needs are not met by current, most common approaches.

\chapter{Design}

\section{Overview}

The Voice Work Record Application is designed to provide a scalable and efficient solution for employees to log and manage their work records using audio-based logging and manual entry. The intended business model is intentionally different from typical solutions. Instead of using a subscription model, the software is meant to be distributed as a full package. This approach increases setup time and complexity for businesses, as all backend services the app depends on need to be setup separately. It also eliminates the need for subscription fees. This also nearly removes the implementation considerations regards setting up for multiple companies, so the simplicity was a factor as well.

A fundamental design decision was to offload authentication and user management to Microsoft accounts. This is partly due to a requirements by DevBalance, as their infrastructure is based around Microsoft account. Since the application is expected to be used across different companies, managing user accounts internally would introduce unnecessary complexity. By leveraging Microsoft's identity management services, authentication and access control are handled externally, ensuring a seamless and secure login experience while eliminating the need for manual account management.

The backend of the system is entirely cloud-based, ensuring high availability, real-time synchronization, and most importantly, strong scalability in exchange for possible fees up the road. A \gls{NoSQL} cloud database stores all timesheet\footnote{Timesheet refers to a structured object in the database that does not directly include all data, often only references to other objects or files. From the view of user, there are only work records (or work entries) that include all data} entries. Audio recordings are also uploaded to cloud storage, making them accessible from both mobile and web applications. Various approaches were considered, including self-hosted database solutions such as MongoDB or SQLite with a shared repository layer. While these could provide flexibility in local or hybrid deployments, they introduce challenges such as scalability limitations, synchronization complexity, and increased maintenance overhead. A cloud-first approach was chosen to provide automatic scaling, seamless access across devices, and minimal operational burden.

A key feature of the application is converting audio logs into structured timesheets. This is achieved through external Open \gls{API} endpoints that handle both speech-to-text transcription and text-to-timesheet parsing. Running a lightweight \gls{AI} model directly on mobile devices was considered as an alternative, but this approach was ultimately discarded due to performance constraints, difficulty in maintaining \gls{AI} models across different devices, and inconsistent processing power across smartphones. By utilizing cloud-based \gls{AI} services, the system ensures accurate and scalable transcription while keeping the front-end client relatively thin.

The balance between thin and thick client models has been a crucial point of architectural consideration while coming up with the entire design. While the application adopts many principles of a thin client — minimizing local processing and relying on cloud-based services for compute-intensive tasks like \gls{AI} transcription and timesheet parsing — it is not purely thin in the traditional sense. In today’s mobile-first landscape, most applications exist on a spectrum between thin and thick clients, incorporating elements of both to optimize user experience, performance, and maintainability. Local caching, \gls{UI} rendering, and minimal offline capabilities are still handled on the client side to ensure responsiveness and usability. By offloading complex logic to an external server, the system avoids overburdening the client device and ensures consistent behavior across platforms. This hybrid approach enables centralized updates, simplified maintenance, and scalable architecture while retaining enough client-side functionality to support a smooth and interactive user experience \textcite{distributedSystems}.

The primary interface for users is a mobile application, available for both Android and \gls{iOS}. The user experience is designed to be minimalistic and intuitive, prioritizing voice-based interactions and quick data entry. Some \gls{UI} patterns were inspired by the afore-mentioned Clockify, an application with a not too dissimilar goal.  

In addition to the mobile app, there is also a web-based application, which provides an alternative way for users to access their timesheet data. The web application was developed independently by Bc. Ondřej Zelinka as a separate implementation. However, since both applications were meant to be interchangeable from the view of the user, the design and core functionalities of the entire system were discussed before and through their implementation, then aligned to ensure consistency between platforms. While both applications were implemented separately, they share many technical details related to backend services, including transcription models, prompts, database structures, and user group management. This ensures that both platforms provide a unified experience and operate on the same fundamental principles.

In conclusion, the system was designed with a strong emphasis on improving user experience, scalability, and operational efficiency. By utilizing Microsoft accounts for authentication, the application ensures secure and simplified access management, particularly important given the multi-organization use case. Cloud-based storage for data and audio recordings guarantees high availability, scalability, and seamless real-time synchronization across devices. The integration of external \gls{AI} services for transcription offloads resource-intensive processing to a third party, reducing system complexity without introducing a critical backend service that would add additional maintenance and most importantly, further scalability and availability concerns.

\section{System Architecture}

During the implementation part of the thesis, a web interface\footnote{The web application is not a part of this thesis and has been developed by a fellow student, Bc. Ondřej Zelinka} has been developed concurrently with the mobile application. Although developed as separate projects, they share many common features and functionalities. Notably, the \gls{UI}/\gls{UX} designs have been discussed and aligned across both platforms. A shared set of supporting services ensures that both the mobile app and the web interface operate seamlessly, offering a consistent experience. These shared services include the entire Firebase stack, which provides authentication, data storage, and standardization of timesheet data formats. Furthermore, the OpenAI endpoints and associated prompts are employed as common resources for both applications and saved in Firebase storage. This design allows the user to utilize both applications simultaneously with a single account, ensuring a unified experience across platforms.

The system architecture is designed with a clear focus on scalability, efficiency, and cross-platform functionality. After careful consideration of the system requirements, along with some prototyping, it has been decided that the core functionality will be supported by the following components.

\subsection{System Components Overview}

After careful consideration of the system requirements, along with some prototyping, it has been decided that the core functionality will be supported by the following components.

\begin{itemize}
    \item \textbf{Android/\gls{iOS} Application} – A mobile application available on both Android and \gls{iOS} platforms, providing users with access to the system's functionalities.
    \item \textbf{Web Application} – Developed by a fellow student, Bc. Ondřej Zelinka, as a separate project. This web-based platform offers similar functionalities to the mobile application, allowing users to interact with the system via a web browser.
    \item \textbf{OpenAI \gls{API}}
    \begin{itemize}
        \item \textbf{Speech-to-Text Endpoint} – Used for transcribing audio recordings to text.
        \item \textbf{Text-to-Timesheet Endpoint} – Converts the transcribed text into a structured timesheet format.
    \end{itemize}
    \item \textbf{Firebase Services}
    \begin{itemize}
        \item \textbf{Firebase Storage} – Used for storing audio files.
        \item \textbf{Firestore Database} – A \gls{NoSQL} database used to store static content such as prompts, activities, clients, and projects, as well as dynamic data such as timesheets.
        \item \textbf{Firebase Authentication} – A service used for managing user authentication across both platforms.
    \end{itemize}
    \item \textbf{Azure App} – Provides finer control over login procedures via Microsoft accounts, as this is the primary method of authentication, as required by Devbalance.
\end{itemize}

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{assets/diagrams/timesheet_app_component_diagram.drawio.png}
    \caption{High level component diagram of the Voice Work Record Application and all external services}
    \label{fig:timesheet_app_architecture}
\end{figure}

Several additional services and supporting components are used and further discussed in the Deployment chapter at page \pageref{chap:deployment}.

\subsection{Entities Overview}

There are in essence two places that persistently store data.

\begin{itemize}
    \item \textbf{Firestore Database} — a \gls{NoSQL} database at its core, accessed through Firebase’s \gls{API}, which provides a convenient interface along with built-in caching mechanisms to reduce the number of actual and expensive (both in terms of time and finances) reads and writes to the cloud. The database stores documents for timesheets, activities, projects, clients, and prompts. Among these, only timesheets are really considered live data and are updated very frequently. See \ref{fig:entity_diagram} for details on all entities. Note the relationships are omitted by design, as a \gls{NoSQL} database does not enforce relational constraints or schemas in the traditional sense and keeping the entries valid is up to the implementation.
    
    \item \textbf{Firebase Storage} -- used to store audio recordings. Similar to Firestore, it offers an abstraction layer that allows developers to organize files into folder-like structures. In our case, separated by generated owner IDs.
\end{itemize}

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{assets/diagrams/entity-relationship-diagram.drawio.png}
    \caption{Persistent entities in Firestore Database}
    \label{fig:entity_diagram}
\end{figure}

Access to both storage systems is restricted on the application side. Additionally, a set of security rules in Firebase ensures that no user can access data that does not belong to them, as illustrated in Listing \ref{lst:firestore_rules}.

\newpage
\begin{lstlisting}[language=Firestore, caption={Firestore Security Rules}, label={lst:firestore_rules}, floatplacement=H, showstringspaces=false, showstringspaces=false]
rules_version = '2';

service cloud.firestore {
  match /databases/{database}/documents {
    match /{document=**} {
      allow read, write: if request.auth != null;
    }

    match /timeSheets/{timesheetId} {
      allow read, write: if request.auth.uid == resource.data.ownerId;
    }
  }
}
\end{lstlisting}

\chapter{Technologies used}

The choice of technologies directly supports the functional and non-functional requirements, enabling efficient development and a seamless user experience. The key technologies used in the application include Flutter framework for building the front-end application and Firebase for persistent storage, authentication, logging and app distribution.

\section{Flutter}

Flutter is a \gls{UI} framework developed by Google that allows for the development of natively compiled applications for mobile, web, and desktop from a single codebase \cite{flutterGithub}. The Voice Work Record Application uses Flutter to create cross-platform applications primarily for Android and \gls{iOS}, though the framework allows to expand on this should the need arise.

The programming language of choice is Dart, which provides strong typing, asynchronous programming, and fast performance. Dart is not the strongest languages (read as "richest set of features") available – for example, there is no support for true multiple inheritance, unchecked exceptions, macros or any kind of compile-time metaprogramming really. Function and operator overloading is also very limited. Some of these might be an attempt to simplify the language for readability, but most of it is a trade-off, that would compilation into a wide range of different devices. I believe a programming language with a richer set of features would just give too many vectors and possible issues when trying to compile to different languages.

By using Flutter, the application is able to maintain consistent functionality and high performance across different platforms.

The core advantage of Flutter are following:
\begin{itemize}
    \item \textbf{Development speed:} Is unparalleled due to how simple building \gls{UI} is and how well the hot reload feature works. It allows developers to instantly see changes in the code reflected in the application without needing to restart the app.
    \item \textbf{Unified language for \gls{UI} and logic:} Dart is used consistently across both the user interface and business logic layers. While Dart may not be the most powerful language for computationally intensive backend tasks, this is not a limitation in our case, as such workloads are handled by third-party services. This uniformity simplifies both the learning curve and code readability.
    \item \textbf{Good runtime performance:} Flutter applications generally demonstrate strong performance out of the box. In most scenarios, little to no manual optimization is necessary unless the application has highly specific or extreme performance requirements.
\end{itemize}

\section{Firebase}

Firebase is a comprehensive platform for building web and mobile applications, offered by Google. It's well optimized for usage with Flutter and provides a set of tools that simplifies backend development, handling key tasks such as data storage, authentication, and crash reporting.

\subsection{Firestore}

Firestore\footnote{Firestore documentation: https://firebase.google.com/docs/firestore} is a \gls{NoSQL} cloud database that is used in the Voice Work Record Application to store work records and user data. Firestore allows for scalable and flexible storage, offering real-time synchronization of data across all devices. This is crucial for ensuring that the data is always up-to-date across different devices, especially when working offline or in poor network conditions.

Data in Firestore is stored in collections of documents. Each document contains fields, where data is stored as key-value pairs.

\subsection{Firebase Storage}

Firebase Storage\footnote{Storage documentation: https://firebase.google.com/docs/storage} is used for storing and serving large files, such as voice recordings. The Voice Work Record Application uses Firebase Storage to securely store audio files that users upload as part of their work records. The integration of Firebase Storage ensures that these files are available for download and playback in a fast and secure manner.

\subsection{Crashlytics}

Crashlytics\footnote{Crashlytics documentation: https://firebase.google.com/docs/crashlytics} is a powerful crash reporting tool integrated into the Voice Work Record Application to track, report, and fix crashes in real-time. Crashlytics provides detailed reports about the nature and frequency of crashes, allowing developers to quickly identify and resolve critical issues, improving the app's stability.

\subsection{Firebase Authentication}

Firebase Authentication\footnote{Authentication documentation: https://firebase.google.com/docs/auth} is used to manage user sign-in and authentication within the application. The Voice Work Record Application utilizes Microsoft authentication to allow users to sign in securely using their Microsoft credentials. This simplifies the authentication process and ensures that user accounts are managed efficiently.

Firebase Authentication supports a wide range of identity providers, including email/password login, social media logins, and custom authentication methods. The flexibility of Firebase Authentication allows for easy integration of additional identity providers if required in the future.

\chapter{Implementation}

\section{Software Architecture}

Flutter by default does not impose strict guidelines on how code should be structured, nor does it mandate the separation of application layers. Flutter uses Dart as its primary programming language, and what would traditionally be a separate view model—often implemented in a different language—is represented here simply as a special class called a `Widget`. This approach is both an advantage and a disadvantage: it enables rapid development and keeps the technology stack minimal, but it also allows, and in some cases even encourages, poor software architecture practices.

Unlike many other platforms, it is entirely possible to implement the entire business logic and all views of an application in a single `.dart` file. As a result, it is crucial for developers to be disciplined and to establish a coherent software architecture early in the development cycle.

\subsection{MVC Pattern}

The Model-View-Controller (\gls{MVC}) pattern is a foundational architectural pattern for user interface design, originally developed by Trygve Reenskaug for the Smalltalk platform in the late 1970s and later popularized across numerous frameworks and platforms \textcite{fowlerPEAA}.

\gls{MVC} divides the user interface into three distinct roles \textcite{mdnMVC}:

\noindent
\begin{itemize}
    \item \textbf{Model} — Represents the underlying domain data and business logic. It is a non-visual component and ideally independent of any user interface elements. In many systems, this corresponds to domain models or transaction scripts, provided they contain no \gls{UI}-related logic.
    \item \textbf{View} — Handles the visual representation of the model. It is responsible for presenting data to the user in various forms, such as GUI elements or HTML pages, but does not itself modify the data.
    \item \textbf{Controller} — Receives user input, manipulates the model accordingly, and updates the view. It effectively acts as an intermediary between the user's actions and the system's internal state.
\end{itemize}

This separation enhances modularity and testability. In particular, the separation of presentation and model is one of the most fundamental heuristics of sound software design. It allows the same business logic to be reused across multiple interface types—such as graphical interfaces, web front ends, or APIs—without altering the model code. Furthermore, models, being non-visual, are significantly easier to test than views, enabling better automated testing and cleaner development practices.

Another key benefit of this separation is directional dependency: the presentation depends on the model, but the model remains completely unaware of the presentation. This simplifies the model’s design and enables greater flexibility when introducing new views or presentation styles.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{assets/diagrams/mvc.drawio.png}
    \caption[Model-View-Controller pattern]{Model-View-Controller (\gls{MVC}) pattern illustration.}
    \label{fig:mvc-pattern}
\end{figure}

\subsection{Layered Pattern}

The application uses an extended, layered variation of the \gls{MVC} pattern. Since Flutter does not enforce architectural boundaries, maintaining this structure is entirely the developer's responsibility. The key layers are:

\begin{itemize}
    \item \textbf{Model}
    \item \textbf{Service}
    \item \textbf{Controller}
    \item \textbf{View}
\end{itemize}

In some cases not all layers are utilised, particularly in cases in which no data is required, hence missing the model. In other cases there are services which do not have a dedicated controller. This is because it's not a wanted occurence for the developer to directly call these services. In such a case, a more generic controller is provided that calls multiple services internally. This allows for simpler calls in the views, as business logic is unwanted there - and neither are thick boilerplates. See below for further explanation and examples of each layer.

\newpage
\subsubsection{Model layer}

Unlike \gls{MVC}, Model merely defines the objects and data structures used throughout the app. It doesn't carry any logic apart from conversion to and from the database. Listing \ref{lst:client_model} shows how a client is represented in the app.

\begin{lstlisting}[language=Dart, caption={Model layer example on Client class}, label={lst:client_model}, floatplacement=H, showstringspaces=false]
import 'package:json_annotation/json_annotation.dart';

part 'client.g.dart';

@JsonSerializable()
class Client {
  final String id;
  final String name;
  final String address;
  final String dic;
  final String ic;
  final String email;
  final String phone;

  Client({
    required this.id,
    required this.name,
    required this.address,
    required this.dic,
    required this.ic,
    required this.email,
    required this.phone,
  });

  factory Client.fromJson(Map<String, dynamic> json) => _$ClientFromJson(json);

  Map<String, dynamic> toJson() => _$ClientToJson(this);
}
\end{lstlisting}

\subsubsection{Service layer}

Contains the business logic and manages operations such as logging. Both of these can bee seen Listing \ref{lst:auth_service}.

\begin{lstlisting}[language=Dart, caption={Service layer example on Firebase AuthService class}, label={lst:auth_service}, floatplacement=H, showstringspaces=false]
class AuthService with WidgetsBindingObserver {
  FirebaseAuth get instance => FirebaseAuth.instance;

  Stream<User?> get userStream => instance.authStateChanges();

  Future<UserCredential> signInWithMicrosoft() async {
    final provider = OAuthProvider("microsoft.com");
    final credential = await instance.signInWithProvider(provider);
    logger.i("Signed with Microsoft: $credential");
    return credential;
  }

  Future<void> logout(BuildContext context) async {
    User? user = instance.currentUser;
    if (user != null && user.isAnonymous) {
      _deleteAccount();
    } else {
      await instance.signOut();
    }
  }

  void dispose() {
    WidgetsBinding.instance.removeObserver(this);
  }
}
\end{lstlisting}

\subsubsection{Controller layer}

Is the layer directly called from views. It delegates most operations to its corresponding services. In this project, controllers are referred to as \textbf{providers} due to naming conventions associated with the widespread Provider library \textcite{providerPackage}, which builds upon Flutter's \texttt{ChangeNotifier} class \textcite{flutterChangeNotifier}.  
The controllers manage the application's state and provide it to other components—hence the term \textit{Provider}. Listing \ref{lst:auth_provider} shows the \gls{API} available to Widgets, as well as how its corresponding service (refer to Listing \ref{lst:auth_service}) is used. This same pattern is used throughout the app.

\newpage
\begin{lstlisting}[language=Dart, caption={Controller layer example on AuthProvider class}, label={lst:auth_provider}, floatplacement=H, showstringspaces=false]
class AuthProvider with ChangeNotifier {
  final _authService = GetIt.I<AuthService>();
  bool _loggingIn = false;

  bool get loggingIn => _loggingIn;
  Stream<User?> get userStream => _authService.userStream;

  AuthProvider() {
    userStream.listen(_onAuthStateChanged);
  }

  void _onAuthStateChanged(User? user) {
    _updateLoggingState(user);
    notifyListeners();
  }

  void _updateLoggingState(User? user) {
    if (user != null) {
      _loggingIn = false;
    }
  }

  Future<void> signInWithMicrosoft() async {
    _loggingIn = true;
    notifyListeners();
    await _authService.signInWithMicrosoft();
  }

  Future<void> logout(BuildContext context) async {
    await _authService.logout(context);
  }
}
\end{lstlisting}

\newpage
\subsubsection{View layer}

Defines the user interface, with limited visibility into business logic, typically interacting only with controllers. In the case of Flutter, the view layer corresponds directly to Widgets, as can be seen in Listing \ref{lst:login_screen}.

\begin{lstlisting}[language=Dart, caption={View layer example on LoginScreen Widget class}, label={lst:login_screen}, floatplacement=H, showstringspaces=false]
class LoginScreen extends StatelessWidget {
  const LoginScreen({super.key});

  @override
  Widget build(BuildContext context) {
    final authProvider = Provider.of<AuthProvider>(context);

    return Scaffold(
      body: Center(
        child: Row(
          mainAxisAlignment: MainAxisAlignment.center,
          children: [
            SquareImageButton(
              onPressed: () {
                authProvider.signInWithMicrosoft();
              },
              imagePath: "assets/microsoft_logo.png",
              size: 100,
            ),
          ],
        ),
      ),
    );
  }
}
\end{lstlisting}

\subsection{Project Directory Structure}

Organizing a Flutter project's directory structure is pivotal for maintaining scalability and readability. While Flutter does not enforce a specific directory layout, adopting a consistent approach is essential, especially as the application grows in complexity.

\subsubsection{Feature-First Structure}

In a feature-first structure, visualized in Listing \ref{lst:project_directory_feature_first}, the application is organized around business features, with each feature acting as a self-contained module.  
This method is particularly suited to agile teams and large projects.

\begin{lstlisting}[caption={Feature-first directory structure}, label={lst:project_directory_feature_first}, floatplacement=H, showstringspaces=false]
lib/
+-- src/
    +-- features/
        +-- feature1/
        |   +-- presentation/
        |   +-- application/
        |   +-- domain/
        |   +-- data/
        +-- feature2/
        +-- feature3/
\end{lstlisting}

Some of the advantages of this structure are easier navigation, better scalability, and simplified refactoring.  
However, it can introduce code duplication and challenges around shared services if not carefully managed.

\newpage
\subsubsection{Layer-first structure}

An alternative approach is the layer-first structure, visualized in Listing \ref{lst:project_directory_layer_first} Here, the application is divided into distinct technical layers, like data, domain, and presentation, independent of features.

\begin{lstlisting}[caption={Layer-first directory structure}, label={lst:project_directory_layer_first}, floatplacement=H, showstringspaces=false]
lib/
+-- src/
    +-- features/
        +-- presentation/
        |   +-- feature1/
        |   +-- feature2/
        |   +-- feature3/
        +-- application/
        |   +-- feature1/
        +-- domain/
        |   +-- feature2/
        |   +-- feature3/
        +-- data/
        |   +-- feature2/
\end{lstlisting}

Layer-first structures promote separation of concerns and re-usability.  
However, they can complicate refactoring efforts and might not scale as cleanly for feature-heavy applications.

Detailed comparison I've based my decisions on can be found in Andrea Bizzotto's article \textcite{flutterStructure}.

\newpage
In this project, a feature-first structure is used, taking the developer management tax in exchange for better long-term maintainability.

\begin{lstlisting}[caption={Voice Record App directory structure}, label={lst:project_directory_timesheet}, floatplacement=H, showstringspaces=false]
lib/
+-- src/
    +-- features/
        +-- audio_playback/
        |   +-- data/
        |   +-- services/
        |   +-- providers/
        |   +-- ui/
        +-- audio_recording/
        +-- audio_transcription/
        +-- auth/
        +-- settings/
        +-- work_records/
    +-- common/
    +-- core/
    +-- firebase_options.dart
    +-- main.dart
\end{lstlisting}

\subsection{State Management}
There are multiple approaches to handling state management in Flutter. In many frameworks, a specific method of state management is prescribed. Because Flutter supports a wide range of platforms, it is not surprising that state management is largely delegated to plugins and third-party libraries \textcite{flutterStateMgmt}.

Among the available options \textcite{flutterStateMgmt}, three methods are generally considered industry standards:

\begin{itemize}
  \item \texttt{StatefulWidget}'s \texttt{setState} method\footnote{Official \texttt{StatefulWidget} documentation: \url{https://api.flutter.dev/flutter/widgets/StatefulWidget-class.html}}
  \item \texttt{Provider} package\footnote{Provider package on pub.dev: \url{https://pub.dev/packages/provider}}
  \item \texttt{Riverpod} package\footnote{Official Riverpod site: \url{https://riverpod.dev/}}
\end{itemize}

In the Voice Record Application, the \texttt{Provider} package manages states that are or may feasibly be shared across multiple components. The \texttt{ChangeNotifier} class simplifies the process of defining listeners and updating them using \texttt{notifyListeners()}. A simplified version of this approach can be seen in Listing \ref{lst:metadata_provider}.

Meanwhile, \texttt{StatefulWidget}'s \texttt{setState()} is utilized whenever the state is local to a single component, typically when it only affects the current view or user interface. See Listing \ref{lst:edit_work_record_screen} to see how this local state management approach is implemented.

\newpage
\begin{lstlisting}[language=Dart, caption={Provider example for MetadataProvider class}, label={lst:metadata_provider}, floatplacement=H, showstringspaces=false]
/// Metadata provider serves as a common entry point for data that is identical to all work records
class MetadataProvider with ChangeNotifier {
  final ClientsService _clientsService = ClientsService();
  final ActivitiesService _activitiesService = ActivitiesService();
  final ProjectsService _projectsService = ProjectsService();

  List<Client> _clients = [];
  List<Activity> _activities = [];
  List<Project> _projects = [];

  MetadataProvider() {
    _loadMetadata();
  }

  Future<void> _loadMetadata() async {
    try {
      _isLoading = true;
      _hasError = false;
      notifyListeners();

      _clients = await _clientsService.getAllClients();
      _activities = await _activitiesService.getAllActivities();
      _projects = await _projectsService.getAllProjects();
    }
  }
}
\end{lstlisting}

\newpage
\begin{lstlisting}[language=Dart, caption={Stateful example on EditWorkRecordScreen Widget class}, label={lst:edit_work_record_screen}, floatplacement=H, showstringspaces=false]
class EditWorkRecordScreen extends StatefulWidget {
  final TimesheetService timesheetService;
  final Timesheet? timesheet;

  const EditWorkRecordScreen({
    super.key,
    required this.timesheetService,
    this.timesheet,
  });

  @override
  State<EditWorkRecordScreen> createState() => _EditWorkRecordScreenState();
}

class _EditWorkRecordScreenState extends State<EditWorkRecordScreen> {
  late TextEditingController _descriptionController;
  Client? _selectedClient;

  @override
  void initState() {
    super.initState();
    _descriptionController =
        TextEditingController(text: widget.timesheet?.description ?? '');
  }
}
\end{lstlisting}

\newpage
\section{Audio recording to database entry conversion}

After the user records audio and describes the work he has done, there are several steps to convert that into a work record:
\begin{enumerate}
  \item Save audio to Firebase Storage
  \item Transcribe the audio into text
  \item Convert transcribed text into a structured format suitable for parsing -- Relevant information needs to be extracted, such as client, project, time duration and a concise work description.
  \item Parse structured text into a Timesheet
  \item Create a work record and save to Firestore Database
\end{enumerate}

While each individual step is relatively straightforward and can be performed with a reasonable degree of reliability, each also introduces potential sources of error -- such as inaccurate transcription, improper text conversion, or failures during parsing. Therefore, a more systematic and robust approach is necessary to ensure the overall integrity of the process.

\subsection{How conversion is evaluated}

To address this challenge systematically, a Python-based testing suite called \texttt{timesheet\_models\_tester} was developed. Take note this is not the primary result of the thesis, rather an optimization tool. Nevertheless, the code is provided as an attachment. This script automates the process of evaluating prompt effectiveness by comparing model-generated outputs with annotated ground-truth data. Each test case consists of an audio file and a corresponding expected output (annotations), which together define what the ideal model response should be.

The audio recordings used are recorded by different people using different microphones. The length and quality of the recordings differ greatly, as does the level of detail provided. Some go quite in depth and are explicit on what project the work has been part of, some do not mention it at all. Most do have the information for which client the work has been done and how long the employee has worked for—these two attributes are key to creating a work record, as in a real world scenario, these will determine how much money the client should pay. The incompleteness is by design—as it was found out during testing, it's quite a common occurrence for the user to forget to mention some critical piece of information.

The suite processes each audio input through transcription, followed by structured prompt-based extraction. The model's responses are then compared to the expected results using the \texttt{SequenceMatcher} class from Python’s standard \texttt{difflib} module \textcite{pythonDifflib}. This class computes a similarity score between sequences, which is particularly useful for evaluating how closely the model's output matches the annotated ground truth.

\texttt{SequenceMatcher} operates by finding the longest contiguous matching subsequence between two inputs, excluding so-called “junk” elements\footnote{\texttt{SequenceMatcher} documentation: \url{https://docs.python.org/3/library/difflib.html\#difflib.SequenceMatcher}}. It calculates a similarity ratio ranging from 0 to 1, where values closer to 1 indicate stronger similarity. For instance, a ratio above 0.6 typically implies a reasonably good match. This allows us to quantify the accuracy of the extracted values in a way that tolerates minor textual deviations, such as synonyms or rephrased fragments, while still penalizing semantically incorrect predictions.

This metric is applied field-by-field for each Timesheet attribute, and an overall accuracy score is computed as the average of the individual field scores. This score is further referred to as \emph{End-to-end \glsdisp{AA}{Average Accuracy}}.

\subsection{Optimizing Transcription Requests}

For our transcription functionality, we evaluated different models available via the OpenAI \gls{API}, specifically \texttt{whisper-1}\footnote{\url{https://platform.openai.com/docs/models/whisper-1}} and \texttt{gpt-4o-transcribe}\footnote{\url{https://platform.openai.com/docs/models/gpt-4o-transcribe}}. The goal was to identify a solution that balances accuracy, language support, and performance for our use case.

The \texttt{whisper-1} model demonstrated reasonably accurate results for English audio transcriptions out of the box. Its performance with Czech audio was less consistent. While most outputs were legible, grammatical inaccuracies were occasionally present, especially in more complex or noisy recordings. It should be noted that poor audio quality significantly impacted transcription accuracy—though in such cases, even annotating the audio for testing was difficult.

Given that the application is primarily intended for Czech-speaking users, this presented a potential usability concern. To address this, we evaluated \texttt{gpt-4o-transcribe} , which offered significantly improved performance in Czech, while maintaining strong English transcription capabilities. The model consistently produced well-structured output with fewer grammatical errors across various audio conditions.

However, the \texttt{gpt-4o-transcribe} model would always attempt to clean up the transcription too much and transform unknown words (for example, names of companies) into similar Czech words. The transcription would be overall better and more understandable to the human eye, but upon testing it was found out that this less word-for-word behaviour actually hurts the end-to-end \gls{AA} for conversion to database entries.

In terms of processing time, both models performed similarly in our tests. 

\shorthandon{-}

\begin{table}[H]
\centering
\begin{tabularx}{\textwidth}{|X|X|X|}
\hline
\textbf{Criterion} & \textbf{whisper-1} & \textbf{gpt-4o-transcribe} \\
\hline
\textbf{Czech} & Occasional grammatical issues & Fluent output at cost of accuracy \\
\hline
\textbf{English} & High accuracy & High accuracy \\
\hline
\textbf{Unusual words} & Preserves original wording & Normalizes unknown words incorrectly \\
\hline
\textbf{Readability} & Adequate, occasionally less fluid & Very readable and well-formed \\
\hline
\textbf{\gls{AA}} & Higher due to more literal transcription & Lower due to over-cleaning of transcription \\
\hline
\textbf{Speed} & Fast & Fast \\
\hline
\textbf{Cost} & Lower & Higher \\
\hline
\end{tabularx}
\caption{Comparison of transcription models evaluated for the needs of the Voice Work Record Application}
\label{tab:transcription_models}
\end{table}

Despite the superior quality of \texttt{gpt-4o-transcribe}, we opted to use \texttt{whisper-1} for the final release due to its lower operational cost and better end-to-end \gls{AA}.

Overall, model selection for audio-to-text transcription should consider the target user language, audio quality, and cost-performance trade-offs.

\subsection{Optimizing text-to-timesheet prompts}

In the app, prompt engineering was critical to accurately transform transcribed audio into structured Timesheet data. The challenge of supporting multiple languages, particularly English and Czech, required careful handling of transcription metadata. If the spoken language and metadata did not match, the model would attempt to transcribe and translate, leading to suboptimal results.

To avoid this, we created separate optimized prompts for English and Czech. These prompts ensure accurate transcription by setting a clear language context, enabling the model to focus on transcribing rather than translating. This approach has been key to improving transcription quality and minimizing errors. The prompts were crafted according to best practices for prompt engineering, such as providing clear instructions and limiting ambiguity. For instance, when transcribing in Czech, the prompt includes a direct instruction to interpret and format the text in a specific, structured manner, minimizing the model’s cognitive load. This specificity helps in reducing errors that might arise due to incorrect translations or misunderstood context, especially in professional terms related to time tracking and work descriptions \textcite{openaiPrompt}.

Furthermore, we used specific formatting to improve the model’s response consistency. By carefully choosing appropriate instructions and ensuring that the language metadata matched the transcription language, we were able to optimize the prompts for both languages. This resulted in a significant improvement in the accuracy of the data transformation process, with reduced discrepancies between spoken language and expected results.

For the initial stages of development, the \texttt{gpt-3.5-turbo}\footnote{\url{https://platform.openai.com/docs/models/whisper-1}} model was used. In a later phase of the development cycle, this model was designated as legacy (though it remains accessible via the \gls{API}). Its intended replacement is the \texttt{gpt-4o-mini}\footnote{\url{https://platform.openai.com/docs/models/gpt-4o-transcribe}} model. Preliminary testing with \texttt{gpt-4o-mini} indicated a slight reduction in end-to-end \gls{AA}; however, the overall impact was not significant.

A detailed analysis of attribute-level accuracy revealed that the primary discrepancy stemmed from differences in how textual descriptions were generated. This variation is inherently subjective, as descriptions are not selected from predefined categories (unlike attributes such as client or project), but are instead written freely by the annotator of the test data.

\begin{table}[H]
\centering
\begin{tabularx}{\textwidth}{|X|X|X|}
\hline
\textbf{Criterion} & \textbf{gpt-3.5-turbo} & \textbf{gpt-4o-mini} \\
\hline
\textbf{Attribute extraction accuracy} & Slightly higher & Slightly lower \\
\hline
\textbf{Ambiguous input} & Conservative; more likely to return \gls{N/A} & More likely to guess based on weak hints \\
\hline
\textbf{Description generation} & More consistent & Greater variation in phrasing \\
\hline
\textbf{Performance variability at optimal temperature} & Low & Slightly higher \\
\hline
\textbf{Operational cost} & Higher & Lower \\
\hline
\textbf{Future viability} & Legacy & Intended successor \\
\hline
\end{tabularx}
\caption{Comparison of language models used for text-to-timesheet transformation}
\label{tab:text_to_timesheet_models}
\end{table}

Given that prompt and temperature optimization had already been completed for \texttt{gpt-3.5-turbo}, it was chosen for the final version. Nonetheless, there are plans to transition to \texttt{gpt-4o-mini} in the future, as it offers comparable performance with lower associated costs.

\subsubsection{Prompt optimization}

During the development of the timesheet transcription and extraction system, particular attention was paid to prompt optimization. While transcription using OpenAI's \texttt{whisper-1} model produced usable results from the start, extracting structured information from the transcription — specifically client, project, activity, duration, and description — required more nuanced tuning of prompts \textcite{openaiPrompt}.

\subsubsection{Temperature optimization}

A significant part of the prompt optimisation effort focused on selecting the right temperature setting. Temperature is a crucial parameter in the behavior of large language models, including those provided by OpenAI. It directly influences the randomness or determinism of the model's responses.

In simple terms, temperature controls how creative or conservative the model is when generating text. Lower values (closer to 0.0) make the model more deterministic — it tends to return the most likely and predictable output. Higher values (up to 1.0 or beyond) encourage more randomness and variation, allowing for more diverse or creative completions but also increasing the likelihood of less relevant or inconsistent outputs.

For tasks requiring precise structure and reliability — such as extracting well-defined fields from a transcript — lower temperatures are generally preferred. Overly low temperatures may also reduce the model's ability to generalize to slightly ambiguous or unconventional input phrasings. Conversely, higher temperatures may help the model interpret more loosely defined prompts, but at the cost of consistency and factual accuracy.

In this project, a range of temperature values was tested using the \texttt{timesheet\_models\_tester} script. These ranged from 1.0 (high randomness) to 0.0 (fully deterministic), with intermediate steps at 0.75, 0.5, and 0.25. This allowed for a systematic evaluation of how temperature impacts both extraction accuracy and response stability.

\begin{figure}[ht]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            width=0.8\textwidth,
            height=7cm,
            grid=both,
            xlabel={Temperature},
            ylabel={\gls{AA} (\%)},
            ymin=70, ymax=85,
            xtick={0.0, 0.25, 0.5, 0.75, 1.0},
            ytick={70, 75, 80, 85},
            title={\gls{AA} over 8 runs with gpt-3.5-turbo},
            mark size=2pt,
            every axis plot/.append style={thick},
        ]
        \addplot[
            color=blue,
            mark=o,
        ] coordinates {
            (1.0, 78.3)
            (0.75, 77.3275)
            (0.5, 78.1825)
            (0.25, 80.5575)
            (0.0, 79.55)
        };
        \end{axis}
    \end{tikzpicture}
    \caption{Relationship between model temperature and average end-to-end \gls{AA} (audio to database entry) of timesheet extraction.}
    \label{fig:temp_vs_accuracy}
\end{figure}

Fine-tuning this parameter helped strike a balance between robustness and precision, ensuring that the model could handle varied transcription inputs without deviating too far from the expected timesheet format \textcite{openaiTemp}.

Given the nature of requests made to the endpoints, it was expected the lowest temperature settings at 0.0 to 0.1, i.e. the most deterministic one, would have the best overall results. This was not the case, as can be seen in the Figure \ref{fig:temp_vs_accuracy} and Table \ref{tab:temperature_summary}. Upon investigating the results, the reason is the following: Quite often the user would not mention all of necessary information - very often the activity and/or the project. These are both chosen from a pre-defined long list of options so it's no surprise the user doesn't exactly specify. Now the low requests with low temperature would often produce an \gls{N/A} If the attribute was not explicitly mentioned. This is OK behaviour, but in many cases, from a human point of view at the very least, it was possible to guess the correct option based on the description of the task. For example, If the user mentioned in the recording that he was working with databases, it's feasible the project could fit into the "Storage" category. While the higher tempereture requests would differ across multiple runs, the guess would sometimes be correct. Choosing too high of a temperature caused the responses to always try to fit into some option rather than choosing a more generic one. That behaviour would often overflow in choosing seemingly random options just for the sake of choosing one.

Temperature of around 0.25 was chosen to be the most reliable one - it guessed correctly often enough, but it's guesses weren't too rash or random. This provided the highest overall end-to-end \gls{AA} of 80.56\% and a reasonable \gls{RSD} of 3.44\%. The results can be seen in Table \ref{tab:temperature_summary}.

Interestingly, the lowest deviation was observed at a temperature setting of 0.5. This may be attributed to its proximity to the default value of 0.6, suggesting that the model could be better optimized for this range on OpenAI's side.

\begin{table}[H]
  \centering
  \begin{tabularx}{\textwidth}{lXXX}
    \toprule
    \textbf{Temperature} & \textbf{\gls{AA} (\%)} & \textbf{\gls{SD}} & \textbf{\gls{RSD} (\%)} \\
    \midrule
    1.00  & 78.30 & 4.51 & 5.76 \\
    0.75  & 77.33 & 4.38 & 5.67 \\
    0.50  & 78.18 & 2.04 & 2.62 \\
    0.25  & 80.56 & 2.77 & 3.44 \\
    0.00  & 79.55 & 2.38 & 2.99 \\
    \bottomrule
  \end{tabularx}
  \caption{Summary of \gls{AA}, \gls{SD}, and \gls{RSD} for various temperature settings.}
  \label{tab:temperature_summary}
\end{table}

\newpage
Figure \ref{lst:user_prompt_template_en} shows the english version of user prompt template used in the application. Some parts are naturally dynamic and each request differs. Clients, activites and projects are pre-defined. Text to be processed is provided from response to the audio transcription request.

\begin{lstlisting}[caption={English version of the user prompt template used in the application}, label={lst:user_prompt_template_en}, floatplacement=H, showstringspaces=false]
Transform the text given below into a CSV row semicolon separated in this exact format: CLIENT;ACTIVITY;PROJECT;TIME SPENT;WORK DESCRIPTION

Instructions for the transformation:
* Always try to fill out all attributes. If it is not possible to extract from the text, use a generic option if available. If not, use "N/A"
* Use only following values in the CLIENT field: {clients}
* Use only following values in the ACTIVITY field: {activities}
* Use only following values in the PROJECT field: {projects}
* Convert the described work duration into hours and minutes
* The time format for TIME SPENT field is HH:MM (HOURS:MINUTES)
* Summarize the task into a WORK DESCRIPTION field, maximum 5 words. Make sure the description makes sense and is grammatically correct.
* Do not include any additional explanations or text in the output

Text to process: {transcribed_text}
\end{lstlisting}

\chapter{Deployment}
\label{chap:deployment}

In the software development life-cycle, after the application is developed and thoroughly tested, the next step is deployment. This chapter outlines how the Android app, developed as part of this thesis, is prepared for deployment and the technologies used to support the process.

The app uses a \gls{CI}/\gls{CD} pipeline built on GitLab, to automate the help with the development process.  While the \gls{CI}/\gls{CD} pipeline automates some continuous integration steps, the actual deployment to Google Play and/or Firebase App Distribution is a manual step.

\section{CI/CD}

In modern software development, \textbf{Continuous Integration (\gls{CI})} and \textbf{Continuous Deployment (\gls{CD})} are essential practices \textcite{gitlabCI}. \gls{CI} ensures that code changes are automatically integrated, tested, and built, while \gls{CD} extends this to automate the deployment process.

For this project, the \gls{CI}/\gls{CD} pipeline is managed via \textbf{GitLab} and automates several stages of the development process:

\begin{itemize}
  \item \textbf{Lint}: Ensures code quality and style guidelines are followed.
  \item \textbf{Version control}: Automatically updates version information in the \texttt{pubspec.yaml} file.
  \item \textbf{Test}: Runs unit tests to verify functionality and catch errors early.
  \item \textbf{Build}: Compiles the Android app into a release \gls{APK}.
\end{itemize}

Although deployment to \textbf{Google Play} and \textbf{Firebase App Distribution} is not fully automated, the pipeline prepares the app for deployment by automating the build, testing, and versioning processes. Once the build is completed, \gls{APK} from build artefacts can be used to release the new version.

\section{Version control}

Versioning plays a crucial role in the management and delivery of software, particularly in collaborative and iterative development environments. It provides a structured way to track and communicate changes, enhancements, and fixes, thus facilitating better coordination among team members and stakeholders, and ensuring consistency across distributed versions of the software.

In the development of the smart timesheet app, we adopted the semantic versioning standard, a widely recognized system designed to convey meaning about the underlying code with each version identifier \textcite{semver}. The standard uses the format \texttt{MAJOR.MINOR.PATCH}, where:

\begin{itemize}
  \item \texttt{MAJOR} version increases indicate incompatible \gls{API} changes,
  \item \texttt{MINOR} version increases add functionality in a backward-compatible manner, and
  \item \texttt{PATCH} version increases represent backward-compatible bug fixes.
\end{itemize}

In our project, version numbers follow the following pattern:

\begin{verbatim}
MAJOR.MINOR.PATCH+BUILD_METADATA
\end{verbatim}

To give a clearer example:

\begin{verbatim}
0.11.0+1745075692
\end{verbatim}


The core \texttt{0.11.0} adheres to the semantic versioning scheme, while the suffix \texttt{+1745075692} serves as build metadata. This metadata, represented as a Unix timestamp, enables differentiation of individual builds down to the level of Git commits, which is particularly helpful for internal testing and continuous integration workflows. A Unix timestamp is a numeric representation of the number of seconds that have elapsed since January 1, 1970 (midnight \gls{UTC}/\gls{GMT}), and is commonly used in computing for time tracking \textcite{mdnUnixTime}.

The build metadata is automatically appended by a GitLab \gls{CI} pipeline, which injects a timestamp. This automation ensures that each distributed version can be traced back to the exact source state, facilitating debugging, auditing, and reproducibility.

Development leading up to the final delivery of this thesis is considered work-in-progress. The first stable release — version \texttt{1.0.0} — is planned to coincide with the project's completion and will signify a stable, production-ready build \textcite{semver}.

\section{Crashlytics}

To ensure application stability and improve maintainability, the Voice Work Record Application integrates \textbf{Firebase Crashlytics} for real-time crash reporting and error logging. This system enables developers to detect, analyze, and respond to runtime issues as they occur in production environments, significantly reducing debugging time and enhancing the end-user experience.

\subsection{Integration with Firebase Crashlytics}

Firebase Crashlytics is a real-time crash reporter that helps track, prioritize, and fix stability issues. The integration in this application follows the official Firebase guidelines for Flutter \textcite{firebaseCrashlyticsStart}. It includes setup in the application’s initialization logic to capture both Flutter and platform-level errors.

The following function configures Flutter’s error handling to forward uncaught exceptions to Crashlytics:

\begin{lstlisting}[language=Dart, caption={Firebase Crashlytics configuration side of the app}, label={lst:error_recording}, floatplacement=H, showstringspaces=false]
/// Setup to send fatal crashes to Firebase Crashlytics
void setupErrorRecording() {
  FlutterError.onError = (FlutterErrorDetails details) {
    logger.e('Recording error: ${details.exception}');
    FirebaseCrashlytics.instance.recordFlutterFatalError(details);
  };
}
\end{lstlisting}

This setup ensures that all uncaught Flutter framework errors are reported to Firebase Crashlytics and simultaneously logged locally for immediate visibility during development.

In addition to exception reporting, Crashlytics supports the logging of custom messages. This allows developers to capture useful context about the application’s state, even when errors do not cause a crash. Crashlytics is primarily a convenience tool that simplifies monitoring and post-mortem analysis in production environments - it does not by any means replace common logging practices.

To take advantage of both Crashlytics and common logging practices, an in-app logging system is implemented. It collects diagnostic messages throughout the session and is designed to pass the accumulated logs to Crashlytics automatically in the event of an exceptions error. Other information, such as the user identifier (if one is signed in) is logged as well. See Listing \ref{lst:logger_setup} to see how logging is setup using the singleton pattern.

\begin{lstlisting}[language=Dart, caption={Logger setup with custom Crashlytics output}, label={lst:logger_setup}, floatplacement=H, showstringspaces=false]
import 'package:logger/logger.dart';
import 'package:smart_timesheet_app_mobile/common/crashlytics_output.dart';

Logger get logger => Log.instance;

class Log extends Logger {
  Log._()
      : super(
          output: MultiOutput([
            ConsoleOutput(),
            CrashlyticsOutput(),
          ]),
        );

  static final instance = Log._();
}
\end{lstlisting}

A hidden developer screen -- accessible by tapping on the app version number (as is common practice), displays all locally collected logs. This allows testers or users to manually inspect and share logs with developers, even if no exception was caught by the Flutter crash handler.

\section{App Distribution}

To facilitate real-world testing during development, the application was distributed using \textbf{Firebase App Distribution}, a service within the Firebase suite that simplifies sharing pre-release builds of Android apps with testers.

Firebase App Distribution allows developers to quickly release versions of the app to selected testers without going through the full Google Play release process. This makes it ideal for iterative development, bug tracking, and gathering feedback in short cycles.

Over the course of development, approximately 20 builds of the app were distributed to testers. These builds included incremental updates based on test feedback, bug fixes, and newly implemented features. Testers received updates via email or Firebase App Tester, where they could install the \gls{APK} directly onto their devices. Testing of the \gls{iOS} version has been limited—only by the developer.

This method ensured efficient feedback loops via the App Tester app, which allows users to conveniently provide feedback directly to the developer.

\chapter{Conclusion and Future Work}

This thesis aimed to develop an application for logging work hours, initially created in collaboration with the company DevBalance. The goal of the project was to build an application that could effectively track work hours, provide insights, and be deployed as a service to other companies. The system was designed to meet the needs of the company by offering a robust, scalable solution. Throughout the development process, we ensured that the application was intuitive for users, reliable, and capable of handling multiple concurrent users.

While the project successfully achieved its objectives, and the app was implemented as planned, the collaboration with DevBalance will not extend beyond the scope of this thesis. Although the app was originally intended to be a product offered by DevBalance to other businesses as a subscription service, the company has decided to discontinue this direction. As a result, the application is now fully open-source and will be made available to the public for free.

Moving forward, the app will be adapted for a broader user base. The company-focused version will stay, but the business model will be adjusted to simplify registration and usage by multiple companies or small teams. Plans are in place to deploy it on Google Play for easy access.

The application was developed using Flutter, which allowed for a consistent experience across multiple platforms. The backend is powered by cloud technologies, ensuring scalability and reliability for the app’s users. The deployment was carried out using Firebase, which provided a robust infrastructure for managing data and user interactions.

Not to say the implementation is without challenges. Namely, low test coverage and limited testability of certain classes. The \gls{UI}/\gls{UX} is well-developed, but would benefit from broader user testing to identify potential issues and improvements.

In conclusion, the work presented in this thesis provides a fully functional time logging application that is now open-source and used in production by a small number of users. It shows without a doubt that the outlined transcription use case is a valid one but does not, like all \gls{AI} solutions, guarantee 100\% correctness in all cases.

While the original business goal of the project was to create a commercially viable product, the shift to an open-source model provides opportunities for the broader community to benefit from and contribute to the app.

\shorthandon{-}

\appendix %% Start the appendices.
\chapter{Application Source Code}
A ZIP archive is provided with this thesis containing a snapshot of the application repository as it stood at the conclusion of this work. The project adheres to a standard Flutter project structure and was developed entirely from scratch. The entire codebase represents my original work.

It is possible to build and run the application separately, but will require setup and configuration of the following components:
\begin{itemize}
  \item OpenAI \gls{API} key
  \item Android and/or \gls{iOS} Firebase project
  \item Microsoft Azure App Registration (optional, only required If the developer wishes to have control over who logs in)
\end{itemize}

\chapter{Prompt Optimization Testing Script Source Code}
Additionally to the application source code, a ZIP archive of the repository is included that contains the Python script used for prompt optimization. The repository also contains annotated audio recordings used during testing. The source code is also available in a public GitHub repository\footnote{
\url{https://github.com/Crytastic/timesheet_models_tester}
}.

\chapter{Screenshots of the Application}

\begin{center}
\begin{minipage}{0.45\textwidth}
  \begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{assets/login_screen.png}
    \caption{Login screen of the application}
    \label{fig:login_screen}
  \end{figure}
\end{minipage}
\hspace{0.05\textwidth}
\begin{minipage}{0.45\textwidth}
  \begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{assets/developer_screen.png}
    \caption{Hidden developer screen}
    \label{fig:developer_screen}
  \end{figure}
\end{minipage}
\end{center}

\vspace{1em}

\begin{center}
\begin{minipage}{0.45\textwidth}
  \begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{assets/recording_screen.png}
    \caption{Recording interface}
    \label{fig:recording_screen}
  \end{figure}
\end{minipage}
\hspace{0.05\textwidth}
\begin{minipage}{0.45\textwidth}
  \begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{assets/records_screen.png}
    \caption{Overview of recorded entries}
    \label{fig:records_screen}
  \end{figure}
\end{minipage}
\end{center}

\vspace{1em}

\begin{center}
\begin{minipage}{0.45\textwidth}
  \begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{assets/edit_record.png}
    \caption{Editing a record}
    \label{fig:edit_record}
  \end{figure}
\end{minipage}
\hspace{0.05\textwidth}
\begin{minipage}{0.45\textwidth}
  \begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{assets/filter_records_by.png}
    \caption{Filtering records by criteria}
    \label{fig:filter_records}
  \end{figure}
\end{minipage}
\end{center}

\vspace{1em}

\begin{center}
\begin{minipage}{0.45\textwidth}
  \begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{assets/group_records_by.png}
    \caption{Grouping records by attributes}
    \label{fig:group_records}
  \end{figure}
\end{minipage}
\hspace{0.05\textwidth}
\begin{minipage}{0.45\textwidth}
  \begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{assets/settings_screen.png}
    \caption{Settings screen}
    \label{fig:settings_screen}
  \end{figure}
\end{minipage}
\end{center}


\end{document}
